<p>
  <span data-role="tag" data-value="5fe8ef7d-fa78-f5ec-773c-cca8f4245c1f" id="idb4bfa505-8612-712a-d545-930d733e4859" data-type="figure">Figure 6</span>&nbsp;illustrates a functional block diagram of an embodiment of 
  <span data-role="tag" data-value="6f1c50e5-95b7-391f-0e74-35cded63077d" id="id7c53092a-e6df-9cac-06fe-d0941d2aff70" data-type="drawingObject">AR device logic 600</span>. The 
  <span data-role="tag" data-value="6f1c50e5-95b7-391f-0e74-35cded63077d" id="id0e4ca7b9-e32d-77dc-15f2-dc15737b6520" data-type="drawingObject">AR device logic 600</span>&nbsp;comprises the following functional modules: a 
  <span data-role="tag" data-value="bcc3a6d2-35bb-67ac-3a0c-e6e4d9b4c397" id="id752067f5-434b-642b-6757-1d888be94e4e" data-type="drawingObject">rendering engine 616</span>,&nbsp;
  <span data-role="tag" data-value="6aede5b7-7f10-bfaf-c136-02dce7ea01bb" id="id3b4355d7-fc56-c689-c9e0-3060057b0f15" data-type="drawingObject">local augmentation logic 614</span>, 
  <span data-role="tag" data-value="3e07947d-8633-7a0e-e0c2-de768251ddca" id="id0640d24d-8745-116c-fbdc-61b3f3974a93" data-type="drawingObject">local modeling logic 608</span>, a&nbsp;
  <span data-role="tag" data-value="80e7f3f2-8088-838d-92b9-7b28e2aad348" id="idf223a5a9-9ccd-255c-470b-9aed36213ba0" data-type="drawingObject" data-broken="true">model aggregator 616 (deleted)</span>,&nbsp;
  <span data-role="tag" data-value="67779935-27c7-2bb7-cfd2-6b23b03e7ec9" id="id3953733a-58be-678e-0e92-054a7466f38b" data-type="drawingObject">device tracking logic 606</span>, an 
  <span data-role="tag" data-value="f1c47e74-1e4b-553c-7dbd-ef471b153456" id="idc435aa15-f0a7-8331-7d73-69d2253ddf9a" data-type="drawingObject">encoder 612</span>, and a 
  <span data-role="tag" data-value="282e2ac4-87ae-db3b-2d57-ddd56a33a410" id="id642fdf80-fcf0-77d8-e912-7a3910123348" data-type="drawingObject">decoder 620</span>. Each of these functional modules may be implemented in software, dedicated hardware, firmware, or a combination of these logic types.
</p>
<p>The 
  <span data-role="tag" data-value="bcc3a6d2-35bb-67ac-3a0c-e6e4d9b4c397" id="idd70e5039-c5fc-4b2a-4223-a80c718ff4c7" data-type="drawingObject">rendering engine 616</span> controls the 
  <span data-role="tag" data-value="80dd75cd-34e9-edda-4e86-494e2c71067f" id="id9c38223f-9e69-dffd-c696-368297c1dcf1" data-type="drawingObject">graphics engine 618</span>&nbsp;to generate a stereoscopic image visible to the wearer, i.e. to generate slightly different images that are projected onto different eyes by the optical components&nbsp;of a headset substantially simultaneously, so as to create the impression of 3D structure.
</p>
<p>The stereoscopic image is formed by 
  <span data-role="tag" data-value="bcc3a6d2-35bb-67ac-3a0c-e6e4d9b4c397" id="id94f22a54-3ebb-669f-bfa9-d2439429220c" data-type="drawingObject">rendering engine 616</span>&nbsp;rendering at least one virtual display element (&ldquo;augmentation&rdquo;), which is perceived as a 3D element, i.e. having perceived 3D structure, at a real-world location in 3D space by the user.
</p>
<p>An&nbsp;augmentation&nbsp;is defined by an&nbsp;augmentation object stored in the 
  <span data-role="tag" data-value="5bdee546-48b3-965a-8ebc-fa77811a1aa8" id="idf80d0d24-30e3-0665-7c55-7a0e96b90c1f" data-type="drawingObject">memory 602</span>. The augmentation object comprises: location data defining a desired location in 3D space for the virtual element (e.g. as (x,y,z) Cartesian coordinates); structural data defining 3D surface structure of the virtual element, i.e. a 3D model of the virtual element; and image data defining 2D surface texture of the virtual element to be applied to the surfaces defined by the 3D model. The augmentation object may comprise additional information, such as a desired orientation of the&nbsp;augmentation.
</p>
<p>The perceived 3D effects are achieved though suitable rendering of the&nbsp;augmentation object. To give the impression of the&nbsp;augmentation having 3D structure, a stereoscopic image is generated based on the 2D surface and 3D&nbsp;augmentation&nbsp;model data in the data object, with the&nbsp;augmentation&nbsp;being rendered to appear at the desired location in the stereoscopic image.</p>
<p>A 3D model of a physical object&nbsp;is used to give the impression of the real-world having expected tangible effects on the&nbsp;augmentation, in the way that it would a real-world object. The 3D model represents structure present in the real world, and the information it provides about this structure allows an&nbsp;augmentation&nbsp;to be displayed as though it were a real-world 3D object, thereby providing an immersive&nbsp;augmented&nbsp;reality&nbsp;experience. The 3D model is in the form of 3D mesh.</p>
<p>For example, based on the model of the real-world, an impression can be given of the&nbsp;augmentation&nbsp;being obscured by a real-world object that is in front of its perceived location from the perspective of the user; dynamically interacting with a real-world object, e.g. by moving around the object; statically interacting with a real-world object, say by sitting on top of it etc.</p>
<p>Whether or not real-world structure should affect an&nbsp;augmentation&nbsp;can be determined based on suitable rendering criteria. For example, by creating a 3D model of the perceived AR world, which includes the real-world surface structure and any&nbsp;augmentations, and projecting it onto a plane along the AR user&#39;s line of sight as determined using pose tracking (see below), a suitable criteria for determining whether a real-world object should be perceived as partially obscuring an&nbsp;augmentation&nbsp;is whether the projection of the real-world object in the plane overlaps with the projection of the&nbsp;augmentation, which could be further refined to account for transparent or opaque real world structures. Generally the criteria can depend on the location and/or orientation of the&nbsp;augmented&nbsp;reality&nbsp;device and/or the real-world structure in question.</p>
<p>An&nbsp;augmentation&nbsp;can also be mapped to the mesh, in the sense that its desired location and/or orientation is defined relative to a certain structure(s) in the mesh. Should that structure move and/or rotate causing a corresponding change in the mesh, when rendered properly this will cause corresponding change in the location and/or orientation of the&nbsp;augmentation. For example, the desired location of an&nbsp;augmentation&nbsp;may be on, and defined relative to, a table top structure; should the table be moved, the&nbsp;augmentation&nbsp;moves with it. Object recognition can be used to this end, for example to recognize a known shape of table and thereby detect when the table has moved using its recognizable structure. Such object recognition techniques are known in the art.</p>
<p>An&nbsp;augmentation&nbsp;that is mapped to the mash in this manner, or is otherwise associated with a particular piece of surface structure embodied in a 3D model, is referred to an &ldquo;annotation&rdquo; to that piece of surface structure. In order to annotate a piece of real-world surface structure, it is necessary to have that surface structure represented by the 3D model in question&mdash;without this, the real-world structure cannot be annotated.</p>
<p>The 
  <span data-role="tag" data-value="3e07947d-8633-7a0e-e0c2-de768251ddca" id="id3bc041c8-f298-71ca-c480-d859478cb7a5" data-type="drawingObject">local modeling logic 608</span>&nbsp;generates a local 3D model &ldquo;LM&rdquo; of the environment in the 
  <span data-role="tag" data-value="5bdee546-48b3-965a-8ebc-fa77811a1aa8" id="idfd32ec83-00e4-f633-7b5b-6ed2b7db31e1" data-type="drawingObject">memory 602</span>, using the AR device&#39;s own sensor(s) e.g. 
  <span data-role="tag" data-value="cd130b7e-b8a8-c2b9-2c2f-2ca761a1600e" id="ida427a3fd-d38b-6505-ae9b-0f63b9d9248f" data-type="drawingObject">cameras 610</span>&nbsp;and/or any dedicated depth sensors etc. The 
  <span data-role="tag" data-value="3e07947d-8633-7a0e-e0c2-de768251ddca" id="id425e1bec-4a3d-5621-6c6c-4caefd1a0d64" data-type="drawingObject">local modeling logic 608</span>&nbsp;and sensor(s) constitute sensing apparatus.
</p>
<p>The 
  <span data-role="tag" data-value="67779935-27c7-2bb7-cfd2-6b23b03e7ec9" id="id0f3f20a5-cf9f-b9f9-53da-9fc09172c067" data-type="drawingObject">device tracking logic 606</span>&nbsp;tracks the location and orientation of the AR device, e.g. a headset, using local sensor readings captured from the AR device. The sensor readings can be captured in a number of ways, for example using the 
  <span data-role="tag" data-value="cd130b7e-b8a8-c2b9-2c2f-2ca761a1600e" id="idd9302e38-7fb3-c6d9-bf16-6b293d4954c0" data-type="drawingObject">cameras 610</span>&nbsp; and/or other sensor(s) such as accelerometers. The 
  <span data-role="tag" data-value="67779935-27c7-2bb7-cfd2-6b23b03e7ec9" id="idf08e3850-0b39-4f7d-ca45-606a2c68ce1e" data-type="drawingObject">device tracking logic 606</span>&nbsp;determines the current location and orientation of the AR device and provides this information to&nbsp;the 
  <span data-role="tag" data-value="bcc3a6d2-35bb-67ac-3a0c-e6e4d9b4c397" id="idafe136ae-354d-694f-a31e-414eeaeff42d" data-type="drawingObject">rendering engine 616</span>, for example by outputting a current &ldquo;pose vector&rdquo; of the AR device. The pose vector is a six dimensional vector, for example (x, y, z, P, R, Y) where (x,y,z) are the device&#39;s Cartesian coordinates with respect to a suitable origin, and (P, R, Y) are the device&#39;s pitch, roll and yaw with respect to suitable reference axes.
</p>
<p>The 
  <span data-role="tag" data-value="bcc3a6d2-35bb-67ac-3a0c-e6e4d9b4c397" id="id55698ab7-d409-fa83-5192-2e979b6e4f64" data-type="drawingObject">rendering engine 616</span>&nbsp;adapts the&nbsp;local model based on the tracking, to account for the movement of the device i.e. to maintain the perception of the as 3D elements occupying the real-world, for example to ensure that static&nbsp;augmentations&nbsp;appear to remain static (which will in fact be achieved by scaling or rotating them as, from the AR user&#39;s perspective, the environment is moving relative to them).
</p>
<p>The 
  <span data-role="tag" data-value="f1c47e74-1e4b-553c-7dbd-ef471b153456" id="id91d1a4f3-9369-5556-c2f1-0a0758a9e2df" data-type="drawingObject">encoder 612</span>&nbsp;receives image data from the 
  <span data-role="tag" data-value="cd130b7e-b8a8-c2b9-2c2f-2ca761a1600e" id="id3540ee31-70d0-8216-b727-257e38c0ca61" data-type="drawingObject">cameras 610</span>&nbsp;and audio data from the 
  <span data-role="tag" data-value="b73fa5de-2f0d-f185-3cad-9c51ba1f5264" id="id5e205c78-7190-3250-6d38-d198ec02ce20" data-type="drawingObject">microphones 604</span>&nbsp;and possibly other types of data (e.g., annotation or text generated by the user of the AR device using the 
  <span data-role="tag" data-value="6aede5b7-7f10-bfaf-c136-02dce7ea01bb" id="id429dc39a-7ae5-0e5a-eb6e-bda9d12ea4b9" data-type="drawingObject">local augmentation logic 614</span>) and transmits that infomation to other devices, for example the devices of collaborators in the AR environment. The 
  <span data-role="tag" data-value="282e2ac4-87ae-db3b-2d57-ddd56a33a410" id="idb69d4c1b-ca00-8cda-7828-01d0d3f367da" data-type="drawingObject">decoder 620</span>&nbsp;receives an incoming data stream from other devices, and extracts audio, video, and possibly other types of data (e.g., annotations, text) therefrom.
</p>